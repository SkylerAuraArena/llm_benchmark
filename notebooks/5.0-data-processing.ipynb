{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook pour effectuer les résumés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classiques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Markdown\n",
    "import requests\n",
    "\n",
    "import asyncio\n",
    "import async_timeout\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from tqdm.asyncio import tqdm\n",
    "lock = asyncio.Lock()\n",
    "import time\n",
    "\n",
    "#Langchain and LLMs\n",
    "import json\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain_core.prompt_values import StringPromptValue\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_json={\"topic\":\"internal politic\",\"explanation\":\"It's about Trump.\"}\n",
    "example_json_string=json.dumps(example_json)\n",
    "\n",
    "short_prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Please summarize the following text:\n",
    "        {document}\n",
    "        \"\"\",\n",
    "        input_variables=[\"document\"],\n",
    "    )\n",
    "elaborated_prompt_random = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Read the following text and give a short, clear summary of the main facts or ideas about it.\n",
    "        Use markdown style and bulletpoints.\n",
    "\n",
    "        Text to summarize:\n",
    "        {document}\n",
    "        \"\"\",\n",
    "        input_variables=[\"document\"],\n",
    "    )\n",
    "elaborated_prompt_topic = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Read the following text and give a short, clear summary of the main facts or ideas about it.\n",
    "        Consider this text is related to the topic : {topic}\n",
    "        Use markdown style and bulletpoints.\n",
    "\n",
    "        Text to summarize:\n",
    "        {document}\n",
    "        \"\"\",\n",
    "        input_variables=[\"document\",\"topic\"],\n",
    "    )\n",
    "\n",
    "prompts={\"short\":short_prompt,\"elaborate\":elaborated_prompt_random}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions utilisées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction utilisée pour initialiser la clé API OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiateChatGPT(forceReset=False):\n",
    "    if forceReset:\n",
    "        # Suppression de la variable d'environnement\n",
    "        try:\n",
    "            del os.environ[\"OPENAI_API_KEY\"]\n",
    "            print(\"API key removed from environment.\")\n",
    "        except KeyError:\n",
    "            print(\"API key was not set.\")\n",
    "        \n",
    "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction utilisée pour calculer le prix d'utilisation OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gptPrice(response):\n",
    "    input_token_price_per_million=0.5\n",
    "    input_token=1e6\n",
    "    input_token_price_per_token=input_token_price_per_million/input_token\n",
    "    output_token_price_per_million=1.5\n",
    "    output_token=1e6\n",
    "    output_token_price_per_token=output_token_price_per_million/output_token\n",
    "    input_price=response.usage_metadata['input_tokens']*input_token_price_per_token\n",
    "    output_price=response.usage_metadata['output_tokens']*output_token_price_per_token\n",
    "    total_price=input_price+output_price\n",
    "    return total_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction utilisée pour appeller un appel asynchrone à un llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(chain,doc,timeout=200):\n",
    "    try:\n",
    "        async with async_timeout.timeout(timeout):\n",
    "            response = await chain.ainvoke({\"document\": doc})\n",
    "            return response\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Timeout!\")\n",
    "        # Si le temps est dépassé le résumé sera \"Error timeout\" et on passera au résumé suivant, évite les \"divergences/hallucinations\" des LLMs\n",
    "        return \"Error timeout\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"Error processing request\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction permettant d'initier l'appel à un llm pour effectuer un résumé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def summarize(df,prompts,models=None,start_idx=0,end_idx=None,remote_url=None):\n",
    "\n",
    "    if end_idx is None:\n",
    "        end_idx = len(df)\n",
    "\n",
    "    #Nombre maximal de token en input\n",
    "    num_ctx=4096\n",
    "    #Temperature pour les appels llms\n",
    "    temperature=0\n",
    "    #Temps pendant lequel on attend la réponse du llm, au delà on génère un Erreur timeout et on passe au suivant (ne marche pas pour le remote url)\n",
    "    timeout=200\n",
    "\n",
    "    last_execution_time = None\n",
    "    call_count = 0\n",
    "    limit_per_minute = 3\n",
    "    interval = 60.0 / limit_per_minute\n",
    "\n",
    "    for i in tqdm(range(start_idx, end_idx, 1), desc=\"Processing posts\"):\n",
    "            current_row = df.iloc[i]\n",
    "            #Get the model,prompt type\n",
    "            current_model=df.loc[i,'model_name']\n",
    "            current_prompt_type=df.loc[i,'prompt_type']\n",
    "            current_doc=df.loc[i,'clustered_text']\n",
    "            current_summary=df.loc[i,'summary']\n",
    "            token_Initial=df.loc[i,'token_number']\n",
    "\n",
    "            if models is None or (isinstance(models, list) and current_model in models) or current_model == models:\n",
    "                #Le modèle courant est un des modèles sur lesquel on souhaite effectuer un résumé donc on poursuit\n",
    "                if 'done' not in df.columns or (pd.isna(df.loc[i, 'done']) or df.loc[i, 'done'] != True):\n",
    "                    #La colonne done n'existe pas ou si elle existe son contenu n'est pas renseigné ou n'est pas True, donc on fait des chose\n",
    "\n",
    "                    #Selon le modèle courant on défini le llm utilisé\n",
    "                    if current_model==\"gpt3.5\":\n",
    "                        llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",streaming=False,temperature=temperature,max_tokens=num_ctx)\n",
    "                    else:\n",
    "                        if remote_url:\n",
    "                            llm = ChatOllama(base_url=remote_url, model=current_model,streaming=False,safe_mode=False,verbose=True,temperature=temperature,num_ctx=num_ctx)\n",
    "                        else:\n",
    "                            llm = ChatOllama(model=current_model,streaming=False,safe_mode=False,verbose=True,temperature=temperature,num_ctx=num_ctx)\n",
    "                    \n",
    "                    #On défini la chaine d'appel\n",
    "                    chain = prompts[current_prompt_type.lower()] | llm \n",
    "\n",
    "                    #On réalise l'appel au llm\n",
    "                    try:\n",
    "                        if current_model==\"gpt3.5\":\n",
    "                            if call_count >= limit_per_minute:\n",
    "                                time_elapsed = time.perf_counter() - last_execution_time\n",
    "                                if time_elapsed < 60:\n",
    "                                    print(f\"Rate Limit for gpt3.5 turbo reached => sleep {60 - time_elapsed} seconds\")\n",
    "                                    time.sleep(60 - time_elapsed)\n",
    "                                call_count = 0\n",
    "                            \n",
    "                            last_execution_time = time.perf_counter()\n",
    "                            call_count += 1\n",
    "\n",
    "                        #C'est ici qu'on réalise effectivement l'appel au llm en local, ou sur machine distante ou via openAI\n",
    "                        start_time = time.perf_counter()\n",
    "                        if remote_url:\n",
    "                            #Lors d'un appel distant l'asynchronisme marche mal, on ne bénéficie donc pas de la gestion du timeout, ATTENTION il faut donc garder un oeil sur l'avancement....\n",
    "                            response = chain.invoke({\"document\": current_doc})\n",
    "                        else:\n",
    "                            \n",
    "                            response = await call_llm(chain,current_doc,timeout)\n",
    "                        end_time = time.perf_counter()\n",
    "                        execution_time = end_time - start_time\n",
    "\n",
    "                        async with lock:  # Acquire the lock before modifying the DataFrame\n",
    "                            if isinstance(response, str) and response == \"Error timeout\":\n",
    "                                error_message=f\"Timeout error at : {i} for model {current_model} prompt type {current_prompt_type}\"\n",
    "                                print(error_message)\n",
    "                                df.at[i, 'summary']=\"Error timeout\"\n",
    "                                df.at[i, 'error'] = error_message\n",
    "                                df.at[i, 'done'] = True\n",
    "                            elif isinstance(response, str) and response ==\"Error processing request\":\n",
    "                                error_message=f\"Processing error at : {i} for model {current_model} prompt type {current_prompt_type}\"\n",
    "                                print(error_message)\n",
    "                                df.at[i, 'error'] = error_message\n",
    "                                df.at[i, 'done'] = False\n",
    "                            else:\n",
    "                                df.at[i, 'summary'] = response.content\n",
    "                                if current_model!=\"gpt3.5\":\n",
    "                                    df.at[i, 'duration'] = response.response_metadata['total_duration']/1e9\n",
    "                                    df.at[i, 'ratio'] = response.response_metadata['eval_count']/token_Initial*100\n",
    "                                    df.at[i, 'input_tokens'] = response.response_metadata['prompt_eval_count']\n",
    "                                else:\n",
    "                                    df.at[i, 'duration'] = execution_time\n",
    "                                    df.at[i, 'ratio'] = response.response_metadata['token_usage']['completion_tokens']/token_Initial*100\n",
    "                                    df.at[i, 'input_tokens'] = response.usage_metadata['input_tokens']\n",
    "                                    df.at[i, 'price'] = gptPrice(response)\n",
    "                                df.at[i, 'done'] = True\n",
    "                                df.at[i, 'error'] = np.nan\n",
    "                    except Exception as e:\n",
    "                        # Gestion des erreurs, sauvegarde dans un fichier et levée de l'exception\n",
    "                        error_message = f\"Error processing rows {i} : {e}\"\n",
    "                        print(error_message)\n",
    "                        async with lock:\n",
    "                            df.at[i, 'error'] = error_message\n",
    "                            df.at[i, 'done'] = False\n",
    "                else:\n",
    "                    #La colonne done existe et sont contenu est True, donc on fait rien\n",
    "                    continue\n",
    "            else:\n",
    "                #Le modèle courant n'est pas un des modèles retenu pour générer le résumé, on passe\n",
    "                continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On effectue les résumés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On défini ici les chemins pour le csv d'entrée et celui de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path=\"../data/processed/preclassified_clusters_V2.xlsx\"\n",
    "output_file_path=\"../data/processed/temps/df_with_summaries.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère le dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire l'extension du fichier\n",
    "file_extension = os.path.splitext(input_file_path)[1].lower()\n",
    "\n",
    "# Charger le fichier en fonction de l'extension\n",
    "if file_extension == '.csv':\n",
    "    df = pd.read_csv(input_file_path)\n",
    "elif file_extension == '.xlsx' or file_extension == '.xls':\n",
    "    df = pd.read_excel(input_file_path)\n",
    "elif file_extension == '.json':\n",
    "    df = pd.read_json(input_file_path)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported file extension: {file_extension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On défini ici l'url de la machine distante qui héberge le llm (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pod_id=\"nq1t0ojmvfh9th\"\n",
    "# remote_url=f\"https://{pod_id}-11434.proxy.runpod.net/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On active ou pas l'initialisation de la clé API de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        initiateChatGPT(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On défini la liste des modèles que l'on souhaite utiliser pour générer les résumés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_this_models=[\"llama3.1\",\"mistral\",\"qwen2\",\"gemma2\",\"phi3\",\"gpt3.5\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue les résumés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:   6%|▌         | 137/2400 [00:20<05:30,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 136 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  11%|█▏        | 275/2400 [00:46<06:45,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 274 for model phi3 prompt type Short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  22%|██▏       | 533/2400 [01:06<03:34,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 532 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  31%|███       | 737/2400 [01:26<02:59,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 736 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  36%|███▌      | 857/2400 [01:46<03:10,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 856 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  36%|███▌      | 863/2400 [02:06<04:41,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 862 for model phi3 prompt type Short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  37%|███▋      | 893/2400 [02:26<06:01,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 892 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  50%|█████     | 1205/2400 [03:02<02:51,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1204 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  52%|█████▏    | 1241/2400 [03:22<03:37,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1240 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  55%|█████▌    | 1324/2400 [03:42<03:37,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1323 for model mistral prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  57%|█████▋    | 1361/2400 [04:02<04:19,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1360 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  59%|█████▉    | 1426/2400 [04:22<04:18,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1425 for model mistral prompt type Short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  60%|██████    | 1445/2400 [04:42<05:28,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1444 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  61%|██████    | 1463/2400 [05:02<06:47,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1462 for model phi3 prompt type Short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  65%|██████▍   | 1553/2400 [05:22<04:45,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1552 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  68%|██████▊   | 1625/2400 [05:42<04:05,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1624 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  68%|██████▊   | 1631/2400 [06:02<05:34,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1630 for model phi3 prompt type Short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  69%|██████▉   | 1661/2400 [06:22<05:59,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1660 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  70%|██████▉   | 1672/2400 [06:43<07:33,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1671 for model mistral prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  70%|██████▉   | 1673/2400 [07:02<10:39,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1672 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  71%|███████   | 1697/2400 [07:22<10:08,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 1696 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  86%|████████▌ | 2057/2400 [08:45<01:56,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 2056 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  88%|████████▊ | 2117/2400 [09:05<01:35,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 2116 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  94%|█████████▎| 2249/2400 [09:42<00:44,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 2248 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts:  96%|█████████▌| 2309/2400 [10:15<00:35,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 2308 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts: 100%|██████████| 2400/2400 [10:35<00:00,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout!\n",
      "Timeout error at : 2392 for model phi3 prompt type Elaborate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = await summarize(df,prompts=prompts,models=use_this_models,start_idx=0,end_idx=2400,remote_url=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enregistre les modifs dans un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_file_path,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
