{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\jeanv\\Documents\\GitHub\\jun24_bc_llm\\df_random_summaries_llm_eval.csv')\n",
    "\n",
    "df2 = pd.read_csv(r'C:\\Users\\jeanv\\Documents\\GitHub\\jun24_bc_llm\\df_preclassified_summaries_refFree_refBased.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ratio']=df['ratio'].str.replace('%', '').astype(float)\n",
    "\n",
    "filter_mask=(df['model_name']!=\"gpt3.5\")&(df['ratio']<=150)&(df['duration']<=200)&(df['summary']!=\"Error timeout\")\n",
    "\n",
    "df=df[filter_mask]\n",
    "\n",
    "filter_mask2=(df2['model_name']!=\"gpt3.5\")&(df2['ratio']<=150)&(df2['duration']<=200)&(df2['summary']!=\"Error timeout\")\n",
    "\n",
    "df2=df2[filter_mask2]\n",
    "\n",
    "df2 = df2[df2['error'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate evaluation metrics\n",
    "def calculate_metrics(df):\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        summary = row['summary']\n",
    "        reference = row['ref_summary']\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge_scorer.score(summary, reference)\n",
    "        rouge1 = rouge_scores['rouge1'].fmeasure\n",
    "        rouge2 = rouge_scores['rouge2'].fmeasure\n",
    "        rougeL = rouge_scores['rougeL'].fmeasure\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        reference_tokens = [reference.split()]\n",
    "        summary_tokens = summary.split()\n",
    "        smoothing = SmoothingFunction().method4\n",
    "        bleu = sentence_bleu(reference_tokens, summary_tokens, smoothing_function=smoothing)\n",
    "        \n",
    "        # Calculate Flesch Reading Ease\n",
    "        fre_score = textstat.flesch_reading_ease(summary)\n",
    "        \n",
    "        # Calculate Dale-Chall Readability\n",
    "        dale_chall_score = textstat.dale_chall_readability_score(summary)\n",
    "        \n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'rouge1': rouge1,\n",
    "            'rouge2': rouge2,\n",
    "            'rougeL': rougeL,\n",
    "            'bleu': bleu,\n",
    "            'flesch_reading_ease': fre_score,\n",
    "            'dale_chall_readability': dale_chall_score\n",
    "        })\n",
    "    \n",
    "    # Convert to a dataframe\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    return pd.concat([df, metrics_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframe is 'df' with 'summary' and 'reference_summary' columns\n",
    "df_with_metrics = calculate_metrics(df)\n",
    "\n",
    "df2_with_metrics = calculate_metrics(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
